{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-graham",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "To handle a big block of text we need ways of *segmenting* it into smaller pieces we can tackle.\n",
    "\n",
    "The smallest piece is a *byte* (which depends on the *encoding* of the text), followed by a *character*, and then a *grapheme*.\n",
    "\n",
    "We can segment into multiple characters, or into *words*, up into *sentences* and *sections*.\n",
    "\n",
    "For example if you're analysing a book you may want to break it into chapters, containing sentences, containing words.\n",
    "\n",
    "The right choice of segmentation depends on the *corpus* and the *task* (and your approach).\n",
    "For example if you're using neural networks to translate between languages *subword tokenizers* like BPE ([Senrich, Haddow and Birch, 2015](https://arxiv.org/abs/1508.07909)) or Unigram Language Model ([Kudo 2018](https://arxiv.org/abs/1804.10959)) perform much better than word level segmentation because they can handle new words formed by seen morphemes and suffixes/prefixes.\n",
    "But if you're just trying to understand sentiment you might be better off breaking it into words, normalising them and checking them against a sentiment dictionary.\n",
    "Word segmentation in *unsegmented* languages like Chinese, Japanese, Thai, Balinese, Javanese and Khmer is difficult because even native speakers may not agree on how to break it into words (see [Sproat et al., 1996](https://www.aclweb.org/anthology/J96-3004/) for an example in evaluating Chinese segmentation).\n",
    "\n",
    "Because of this we're going to provide a *range* of segmenters to try with different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-smile",
   "metadata": {},
   "source": [
    "# Normalisation\n",
    "\n",
    "Normalisation is a related activity to segmentation of making things that are different the same.\n",
    "\n",
    "Consider for example in Unicode there are multiple ways of representing the same symbol.\n",
    "There's a [canonical normalisation](http://www.unicode.org/reports/tr15/) called NFC that represents all these tokens the same way.\n",
    "\n",
    "It's often performed at the same time as word segmentation for efficiency, but it is logically a separate step (especially is you keep the context in your tokens like SpaCy does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-patrick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('á', 'á')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "a = u'\\u0061\\u0301'\n",
    "a0 = unicodedata.normalize('NFC', u'\\u0061\\u0301')\n",
    "a, a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-draft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', '́'], ['á'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a), list(a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-party",
   "metadata": {},
   "source": [
    "But for some applications maybe you are just interested in making everything ASCII; this can be useful for e.g. transliterated names.\n",
    "The [unidecode library](https://github.com/takluyver/Unidecode) helps with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.1.2-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 3.9 MB/s eta 0:00:01     |████████████████▍               | 122 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-papua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('北亰', 'Bei Jing ')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"\\u5317\\u4EB0\"\n",
    "x, unidecode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Бизнес Ланч', 'Biznes Lanch')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"Бизнес Ланч\" # Business Lunch\n",
    "x, unidecode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-wrapping",
   "metadata": {},
   "source": [
    "A common example is you may want to unify some kinds of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-skirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('–—-', '----')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '–—-' # hyphen, emdash, endash\n",
    "x, unidecode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-hammer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('¿¡«…» „“', '?!<<...>> ,,\"')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '¿¡«…» „“'\n",
    "x, unidecode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-fairy",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-field",
   "metadata": {},
   "source": [
    "Beyond the character level you may want to treat certain words the same:\n",
    "\n",
    "Spellings: \n",
    "* aluminum vs aluminium\n",
    "* criticize vs critisise\n",
    "\n",
    "Common misspellings:\n",
    "* acceptable vs acceptible\n",
    "\n",
    "Contractions and Abbreviations:\n",
    "* don't for do not\n",
    "* Mr. and Mr for Mister\n",
    "\n",
    "Ideally we would keep the original forms for reference and layer the normalisation on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-beaver",
   "metadata": {},
   "source": [
    "# Tokenization (Word Segmentation)\n",
    "\n",
    "Notable tools:\n",
    "\n",
    "* SpaCy (rule based, a few languages)\n",
    "* Stanza (Neural based, many languages)\n",
    "* Stanford NLP (via Stanza, rule based, a few languages)\n",
    "* NLTK (rule based, a few languages)\n",
    "* [Moses Tokenizer and Normalizer](https://github.com/moses-smt/mosesdecoder/tree/master/scripts/tokenizer) (Perl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-pasta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-judges",
   "metadata": {},
   "source": [
    "!pip install spacy\n",
    "!pip install spacy-lookups-data\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-swing",
   "metadata": {},
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "\n",
    "# From NLTK as referenced in Jurafsky, Speech and Language Processing Chapter 2 (Dec 2020 Draft)\n",
    "\n",
    "TOKENIZE_RE = re.compile(r'''(?x) # set flag to allow verbose regexps\n",
    "     ([A-Z]\\.)+        # abbeviations, e.g. U.S.A\n",
    "    | \\w+(-\\w+)*       # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%? # Currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.           # Ellipsis\n",
    "    | [][.,;\"'?():-_`] # These are separate tokens\n",
    "''')\n",
    "\n",
    "class RegexTokenizer():\n",
    "    def __init__(self, regexp: re.Pattern) -> None:\n",
    "        self.re = regexp\n",
    "        \n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        tokens = []\n",
    "        pos = 0\n",
    "        while pos < len(text):\n",
    "            match = self.re.match(text, pos)\n",
    "            if match:\n",
    "                tokens.append(match.group(0))\n",
    "                pos = match.end()\n",
    "            else:\n",
    "                pos += 1\n",
    "        return tokens\n",
    "\n",
    "def tokenize_space(text: str) -> List[str]:\n",
    "    return re.split(\"\\s\", text)\n",
    "\n",
    "tokenize_ascii = RegexTokenizer(TOKENIZE_RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"That U.S.A. poster-print/photgraph costs $12.40...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-resort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print/photgraph', 'costs', '$12.40...']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_space(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-holiday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'photgraph', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_ascii(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-board",
   "metadata": {},
   "source": [
    "# Subword Segmentation\n",
    "\n",
    "[Sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-democracy",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_data.ipynb.\n",
      "Converted 00_core.ipynb.\n",
      "Converted 01_segment.ipynb.\n",
      "Converted 02_ngram.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
