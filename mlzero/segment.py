# AUTOGENERATED! DO NOT EDIT! File to edit: 01_segment.ipynb (unless otherwise specified).

__all__ = ['RegexTokenizer', 'tokenize_space', 'TOKENIZE_RE', 'tokenize_ascii']

# Cell
from typing import List

# Cell


# Cell
import re

# From NLTK as referenced in Jurafsky, Speech and Language Processing Chapter 2 (Dec 2020 Draft)

TOKENIZE_RE = re.compile(r'''(?x) # set flag to allow verbose regexps
     ([A-Z]\.)+        # abbeviations, e.g. U.S.A
    | \w+(-\w+)*       # words with optional internal hyphens
    | \$?\d+(\.\d+)?%? # Currency and percentages, e.g. $12.40, 82%
    | \.\.\.           # Ellipsis
    | [][.,;"'?():-_`] # These are separate tokens
''')

class RegexTokenizer():
    def __init__(self, regexp: re.Pattern) -> None:
        self.re = regexp

    def __call__(self, text: str) -> List[str]:
        tokens = []
        pos = 0
        while pos < len(text):
            match = self.re.match(text, pos)
            if match:
                tokens.append(match.group(0))
                pos = match.end()
            else:
                pos += 1
        return tokens

def tokenize_space(text: str) -> List[str]:
    return re.split("\s", text)

tokenize_ascii = RegexTokenizer(TOKENIZE_RE)