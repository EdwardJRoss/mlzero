# AUTOGENERATED! DO NOT EDIT! File to edit: 02_ngram.ipynb (unless otherwise specified).

__all__ = ['ngrams', 'T', 'PAD', 'invert_vocab', 'Vocab', 'OOV', 'OOV_IDX', 'PAD_IDX', 'vocab_topn', 'vocab_threshold',
           'count_ngrams', 'ngram_counts_to_conditional_probability', 'product', 'NaiveNgramLanguageModel',
           'flatten_index', 'unflatten_index', 'flatten_index_range', 'csr_top_k_idx', 'SparseRowCubeTensor',
           'NgramLanguageModel']

# Cell
from .segment import *
from .data import *
from typing import *
import bounter

# Cell
T = TypeVar('Token')

PAD = '_PAD_'

def ngrams(tokens: List[T], n: int, pad: T = PAD) -> List[Tuple[T, ...]]:
    """Returns list of ngrams from tokens adding padding as required

    Adds n-1 pad tokens at the start, and 1 to the end
    See https://skeptric.com/ngram-sentence-boundaries/
    """
    if pad is None:
        padded_tokens = tokens
    else:
        padded_tokens = [pad] * (n-1) + tokens + [pad]
    return list(zip(*[padded_tokens[i:] for i in range(n)]))

# Cell
from tqdm.notebook import tqdm

# Cell
def invert_vocab(v):
    return {v: i for i,v in enumerate(v)}

# Cell

OOV = '__OOV__'

OOV_IDX = 1
PAD_IDX = 0

class Vocab:
    def __init__(self, tokenize:Callable[[str], List[str]], tokens: List[str], oov: str=OOV, pad: str=PAD) -> None:
        self.tokenize = tokenize
        assert oov not in tokens
        assert pad not in tokens
        self.i2v = [pad, oov] + list(set(tokens))
        self.v2i = {v:i for i, v in enumerate(self.i2v)}
        self.size = len(self.i2v)

    def encode(self, text: str) -> List[int]:
        return [self.v2i.get(token, OOV_IDX) for token in self.tokenize(text)]

    def decode(self, tokens: List[int]) -> List[str]:
        return [self.i2v[token] for token in tokens]

    def __iter__(self) -> Generator[int, None, None]:
        """Iterate over the numerical tokens"""
        for idx in range(len(self.i2v)):
            yield idx

    def __len__(self) -> int:
        return self.size

    def __repr__(self) -> str:
        return f'Vocab [{", ".join(self.i2v[2:6])}, ...] ({self.size} tokens)'

    def __eq__(self, other: Any) -> bool:
        if isinstance(other, Vocab):
            return self.i2v == other.i2v
        else:
            return False

# Cell
def vocab_topn(corpora: List[str], tokenize: Callable[[str], List[str]], n: int) -> List[str]:
    counts = Counter()
    for doc in tqdm(corpora):
        tokens = tokenize(doc)
        counts.update(tokens)

    ordered_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)
    vocab_tokens = [k for k,v in ordered_counts[:n]]
    return Vocab(tokenize, vocab_tokens)

# Cell
def vocab_threshold(corpora: List[str], tokenize: Callable[[str], List[str]], min_n: int) -> List[str]:
    counts = Counter()
    for doc in tqdm(corpora):
        tokens = tokenize(doc)
        counts.update(tokens)
    ordered_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)
    vocab_tokens = [k for k,v in ordered_counts if v > min_n]
    return Vocab(tokenize, vocab_tokens)

# Cell
def count_ngrams(n:int, vocab: Vocab, corpus: List[str], counter:Optional[Counter[Tuple[int,...]]]=None) -> Counter[Tuple[int,...]]:
    if counter is None:
        counter = Counter()
    for doc in tqdm(corpus):
        tokens = vocab.encode(doc)
        counter.update(ngrams(tokens, n, pad=PAD_IDX))
    return counter

# Cell
from collections import defaultdict

# Cell
def ngram_counts_to_conditional_probability(counts:Counter[Tuple[int, ...]]) -> Dict[int, float]:
    totals = defaultdict(int)
    for ngram, count in counts.items():
        totals[ngram[:-1]] += count

    probs = defaultdict(float)
    for ngram, count in counts.items():
        probs[ngram] = count / totals[ngram[:-1]]
    return probs

# Cell
def product(args):
    total = 1
    for arg in args:
        total *= arg
    return total

# Cell
class NaiveNgramLanguageModel():
    def __init__(self, vocab: Vocab, n: int, corpus:List[str]=None) -> None:
        self.n = n
        self.vocab = vocab
        self.counts = count_ngrams(n, vocab, corpus)
        self.probs = ngram_counts_to_conditional_probability(self.counts)

    def top_k(self, k: int, context: Optional[List[str]] = None) -> Dict[List[str], int]:
        if context is None:
            context = []
        assert len(context) < self.n, f"Context length is greater than model context {self.n}"
        context_tokens = tuple([self.vocab.v2i[t] for t in context])
        l = len(context_tokens)

        filtered = [(v,t) for t,v in self.counts.items() if t[:l] == context_tokens]

        topk_pairs = sorted(filtered, reverse=True)[:k]
        # TODO: Maybe decode should return tuples?
        return {tuple(self.vocab.decode(x[1])): x[0] for x in topk_pairs}

    def probability(self, text: str, pad:bool=True) -> float:
        tokens = self.vocab.encode(text)
        grams = ngrams(tokens, self.n, pad=PAD_IDX if pad else None)
        return product([self.probs[gram] for gram in grams])

    def perplexity(self, text: str, pad:bool=True) -> float:
        tokens = self.vocab.encode(text)
        prob = self.probability(text, pad)
        return prob ** (1/len(tokens))

    def generate(self) -> List[str]:
        tokens = []
        context = (PAD_IDX,) * (self.n-1)
        while True:
            weights = [self.probs[context + (x,)] for x in self.vocab]
            next_token = np.random.choice(len(weights), p=weights)
            if next_token == PAD_IDX:
                break
            tokens.append(next_token)
            context = (context + (next_token,))[1:]
        return self.vocab.decode(tokens)

# Cell
from scipy.sparse import csr_matrix
from sklearn.preprocessing import normalize

# Cell
def flatten_index(ns: List[int], size:int) -> int:
    dim = len(ns)
    return sum(ns[idx] * (size**(dim - idx - 1)) for idx in range(len(ns)))

# Cell
def unflatten_index(n: int, size: int, dim: int) -> List[int]:
    assert size > 1
    ans = []
    for _ in range(dim):
        ans.append(n % size)
        n = n // size
    return list(reversed(ans))

# Cell
def flatten_index_range(ns: List[int], size: int, dim: int) -> slice:
    start = list(ns) + [0] * (dim - len(ns))
    end = list(ns) + [size - 1] * (dim - len(ns))
    start_idx = flatten_index(start, size)
    end_idx = flatten_index(end, size)
    return slice(start_idx, end_idx+1)

# Cell

def csr_top_k_idx(A:csr_matrix, k:int) -> List[Tuple[int, int]]:
    """Returns (row, col) indices for top k values in CSR matrix A"""
    top_ptr_vals = list(A.data.argpartition(-k)[-k:])
    # Find the corresponding row index
    top_rows = [(A.indptr > idx).argmax() - 1 for idx in top_ptr_vals]
    top_cols = A.indices[top_ptr_vals]
    return list(zip(top_rows, top_cols))

# Cell

class SparseRowCubeTensor():
    def __init__(self, data: Dict[Tuple[int, ...], Any], size: int, n_dimension: int, dtype=float) -> None:
        self.size = size
        self.n_dimension = n_dimension

        rows = [flatten_index(t[:-1], size) for t in data]
        cols = [t[-1] for t in data]
        values = list(data.values())

        self.matrix = csr_matrix((values, (rows, cols)),
                                 shape=(size ** (n_dimension - 1), size),
                                        dtype=dtype)

    def __getitem__(self, item):
        if len(item) < self.n_dimension:
            # Should we reshape this??
            return self.matrix[flatten_index_range(item, self.size, self.n_dimension - 1)]
        elif len(item) == self.n_dimension:
            return self.matrix[flatten_index(item[:-1], self.size), item[-1]]
        else:
            raise ValueError(f'Bad dimension {len(item)} expected at most {self.n_dimension}')

    def top_k(self, k:int) -> Dict[Tuple[int,...], Any]:
        top_idxs_flat = csr_top_k_idx(self.matrix, k)
        top_idxs = [tuple(unflatten_index(x[0], self.size, self.n_dimension - 1) + [x[1]]) for x in top_idxs_flat]
        top_values = [self.matrix[idx] for idx in top_idxs_flat]
        return dict(zip(top_idxs, top_values))

    def normalize(self, norm='l1'):
        target = SparseRowCubeTensor(dict(), size=self.size, n_dimension=self.n_dimension)
        target.matrix = normalize(self.matrix, norm=norm, copy=True)
        return target

    def transform(self, f, *args, **kwargs):
        target = SparseRowCubeTensor(dict(), size=self.size, n_dimension=self.n_dimension)
        target.matrix = self.matrix.copy()
        target.matrix.data = f(self.matrix.data, *args, **kwargs)
        return target

# Cell

class NgramLanguageModel():
    def __init__(self, vocab: Vocab, n: int, corpus:List[str]=None, counter:Counter=None) -> None:
        self.n = n
        self.vocab = vocab

        counts = count_ngrams(n, vocab, corpus, counter)
        self.counts = SparseRowCubeTensor(counts, size=len(vocab), n_dimension=n, dtype=int)

        self.probs = self.counts.normalize()

        self.log_probs = self.probs.transform(np.log)


    def top_k(self, k: int) -> Dict[List[str], int]:
        top_grams_counts = self.counts.top_k(k)

        return {tuple(self.vocab.decode(gram)): value for gram, value in top_grams_counts.items()}


    def probability(self, text: str, pad:bool=True) -> float:
        tokens = self.vocab.encode(text)
        grams = ngrams(tokens, self.n, pad=PAD_IDX if pad else None)
        return np.exp(sum([self.log_probs[idx] for idx in grams]))

    def perplexity(self, text: str, pad:bool=True) -> float:
        tokens = self.vocab.encode(text)
        grams = ngrams(tokens, self.n, pad=PAD_IDX if pad else None)
        return np.exp(sum([self.log_probs[idx] for idx in grams]) / len(tokens))

    def generate(self) -> List[str]:
        tokens = []
        context = (PAD_IDX,) * (self.n-1)
        while True:
            weights = self.probs[context].toarray()[0]
            next_token = np.random.choice(len(weights), p=weights)
            if next_token == PAD_IDX:
                break
            tokens.append(next_token)
            context = context[1:] + (next_token,)
        return self.vocab.decode(tokens)